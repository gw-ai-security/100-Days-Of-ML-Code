{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# Day 1: Data Preprocessing\n",
                                     "Updated to current scikit-learn APIs and runnable in VS Code."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Setup in VS Code\\n",
                                     "\\n",
                                     "1) Install Python 3.x and ensure it is on PATH.\\n",
                                     "2) Create and activate a venv in the repo root:\\n",
                                     "\\n",
                                     "```powershell\\n",
                                     "python -m venv .venv\\n",
                                     ".\\\\.venv\\\\Scripts\\\\Activate.ps1\\n",
                                     "```\\n",
                                     "\\n",
                                     "3) Install dependencies in the same environment: \\n",
                                     "\\n",
                                     "```powershell\\n",
                                     "python -m pip install numpy pandas scikit-learn jupyter ipykernel\\n",
                                     "```\\n",
                                     "\\n",
                                     "Then select the `.venv` kernel in VS Code and run the cells.\\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Step 1: Importing the libraries"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import numpy as np\n",
                                     "import pandas as pd\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Step 2: Importing dataset"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "from pathlib import Path\n",
                                     "\n",
                                     "# Resolve datasets/Data.csv relative to the repo root\n",
                                     "cwd = Path.cwd()\n",
                                     "data_path = None\n",
                                     "for parent in [cwd] + list(cwd.parents):\n",
                                     "    candidate = parent / \"datasets\" / \"Data.csv\"\n",
                                     "    if candidate.exists():\n",
                                     "        data_path = candidate\n",
                                     "        break\n",
                                     "if data_path is None:\n",
                                     "    raise FileNotFoundError(\"Could not find datasets/Data.csv. Run the notebook from the repo root.\")\n",
                                     "\n",
                                     "dataset = pd.read_csv(data_path)\n",
                                     "\n",
                                     "X = dataset.iloc[:, :-1].values\n",
                                     "Y = dataset.iloc[:, 3].values\n",
                                     "X, Y\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Step 3: Handling the missing data"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "from sklearn.impute import SimpleImputer\n",
                                     "imputer = SimpleImputer(strategy=\u0027mean\u0027)\n",
                                     "X[:, 1:3] = imputer.fit_transform(X[:, 1:3])\n",
                                     "X\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Step 4: Encoding categorical data"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
                                     "from sklearn.compose import ColumnTransformer\n",
                                     "ct = ColumnTransformer(transformers=[(\u0027country\u0027, OneHotEncoder(), [0])], remainder=\u0027passthrough\u0027)\n",
                                     "X = ct.fit_transform(X)\n",
                                     "labelencoder_Y = LabelEncoder()\n",
                                     "Y = labelencoder_Y.fit_transform(Y)\n",
                                     "X, Y\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Step 5: Splitting the datasets into training set and test set"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "from sklearn.model_selection import train_test_split\n",
                                     "X_train, X_test, Y_train, Y_test = train_test_split(\n",
                                     "    X, Y, test_size=0.2, random_state=0\n",
                                     ")\n",
                                     "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Step 6: Feature Scaling"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "from sklearn.preprocessing import StandardScaler\n",
                                     "sc_X = StandardScaler(with_mean=False)\n",
                                     "X_train = sc_X.fit_transform(X_train)\n",
                                     "X_test = sc_X.transform(X_test)\n",
                                     "X_train, X_test\n"
                                 ]
                  }
              ],
    "metadata":  {
                     "kernelspec":  {
                                        "display_name":  "Python 3",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "name":  "python",
                                           "version":  "3.x"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
